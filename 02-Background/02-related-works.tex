%!TEX root = ../main.tex

% Notes 

\chapter{Related Works}

\section{Hardware Definition Languages (HDL)}

Hardware description languages like Verilog and VHDL are designed for arbitrary circuit description.
Initially, these languages are developed as hardware simulation languages, and were only later adopted as a basis for hardware design.
While HDL language syntaxes resemble software languages but their concepts are inherently different from software languages.
Software programs are designed to describe an algorithm for processors.
They have sequential semantics and correctness of the program is defined as executing instructions in the order the instructions are written.
Data movement is implicit between instructions and there is a explicit defined memory model.
In contrast, HLD languages are designed to specify circuit designs.
Hardware inherently is concurrent and each statement in HDL language is concurrent with other statements.
All the dependencies need to be explicit either in the form of wire or register.
There is no pre-defined memory model. Memory model needs to be explicitly defined and handled in the design.
Because the semantics of these languages are based around simulation, synthesizable designs must be inferred from a subset of the language, complicating tool development and designer education.
HDL languages do not contain a clear semantic just like software languages, as a result, HLD languages can result in ambiguities that make automated synthesis and verification impossible, due to the unclear semantics involved.
Moreover, for any design, in order to achieve maximum generality, they require users to explicitly manage timing, control signals, and local memories.

A big limitation of HDL languages is that these languages lack the powerful abstraction and facilities that are common in modern software languages, which leads to low designer productivity.
For instance, reusability is one of such powerful facilities that HDL languages inherently are aboard to reuse hardware modules~\cite{shacham_rethinking_2010}.
Recent extensions such as SystemVerilog~\cite{systemverilog} improved the type system and parameterized generate facilities but still lack many powerful programming language features.
The key benefit of reusing hardware components is that every time a chip is built, we inherently evaluate different design decisions, either implicitly using microarchitectural and domain knowledge, or explicitly through custom evaluation tools.
Rather than building a custom chip, designers create a template, or a chip generator, that can generate the specialized chip.
For instance, Tensilica applied the same idea to create customized processors~\cite{tensillica}.


Constructing efficient hardware designs requires extensive design-space exploration of alternative system microarchitectures. 
This is another limitation of HLD languages, traditional HLDs have limited support for auto generating modules and are ill-suited to producing and composing the highly parameterized module generators required to support through design-space exploration.

The first step to synthesis behavioral description model to structural structural model is having more powerful hardware languages that can support such important features like enabling hardware reusability, templatizing and hardware modules. In addition, it let's the compiler to efficiently search the design-space for alternative system designs. In the following we look at some of the works to improve and add such features to traditional HLD languages and talk about pros and cons of each approach.

\subsection{BlueSpec}

One alternative proposal to improve productivity of HLD languages is beginning from a domain-specific application programming language and then generate hardware blocks. 
Bluespec~\cite{bluespec} is an example of such languages.
Bluespec Compiler (BSC) is a tool that uses BluespecSystem  Verilog  (BSV)  as  the  design  language.
BSV is a high-level functional HDL based on Verilog and inspired by Haskell, where  modules are implemented as a set of rules using  Verilog  syntax.
In this language, the concurrent behavior of a system is expressed as a collection of rewrite rules.
The rules are called guarded atomic actions and express behavior in the form of concurrently cooperating finite state machines (FSMs).
Rules are predicated with a condition. They give the impression of freezing the rest of the system when a given rule's action is carried out after the rule's predicate is true.
There is an implicit parallelism in this specification, because it is possible for multiple rules to be activated and executed in parallel.
The compiler automatically schedules the rules such that they are either conflict-free or combined sequentially to preserve the promised atomicity semantic.
Please find a counter example, implemented in Bluespec, in  Appendix.

While these can provide great designer productivity when the task in hand matches the pattern encoded in the application programming model, they are a poor match for tasks outside their domain.
For example, the design of a programmable microprocessor is not well described in a stream
programming model, and guarded atomic actions are not a natural way to express a high-level DSP algorithm.
Furthermore, in general it is difficult to derive an efficient microarchitecture from a higher-level computation model, especially if the goal is a programmable engine to run many applications, where the human designer would prefer to write a generator to explore this design space in detail.


\section{Hardware Constructor Languages (HCL)}
\subsection{Genesis2}
Genesis2~\cite{genesis2} was one of the first attempts to work around mentioned limitations, specially reusability. Genesis2 uses Perl language as a macro processing language to provide more flexible parameterization and elaboration of underlying hardware blocks in SystemVerilog. Listing~\ref{listing:genesis2} shows an example of \textit{Genesis2's code}.
In this example, perl is using ceil library from POSIX library to set the values for $\$num\_add\_bits$.

\begin{listing}[ht]
    \begin{minted}{Verilog}
        //; # More Perl Libraries
        //; use POSIX (ceil);
        //; my $reg_list = $self->define_param(REG_LIST => 
        //; [	
        //;     {name => 'regA', width => 5, default => 17, IEO => 'ie'},
        //;     {name => 'regB', width => 15, IEO => 'ieo'},
        //;     {name => 'regC', width => 32, IEO => 'ieo'},
        //; ]);
        //; my $num_regs = scalar(@{$reg_list});
        //; my $num_addr_bits = ceil(log($num_regs)/log(2));

        // Verilog code for the module
        module 'mname()' (
            input                               Clk,
            input                               Reset,
            input ['$num_addr_bits-1':0]        Addr,
            ...
            );

        endmodule // 'mname()'
    \end{minted}
    \caption[Caption for LOF]%
    {Genesis2 code example, combining SystemVerilog and Perl~\cite{genesis2}}
    \label{listing:genesis2}
\end{listing}


These approaches allow familiar and powerful languages like Verilog to be macro languages for hardware netlists, but effectively require leaf components of the design to be described in the underlying HDL.
This combined approach is cumbersome, combining the poor abstraction facilities of the underlying HDL with a completely different high-level programming model that does not understand hardware types and semantics.

\subsection{Chisel and FIRRTL}

Chisel~\cite{chisel} is another successful, modern, generalize HCL which is a domain specific language (DSL) for describing hardware circuits embedded in Scala.
Chisel provides modern programming language features such as meta-programming and object oriented concept coupled with library availability for accurately specifying low-level hardware blocks, but which can be readily extended to capture many useful high-level hardware design patterns.
To see how Chisel could improved the limitations of traditional HDL languages, we try to answer the following question:
\emph{What benefits does Chisel offer over classic Hardware Description Langues?\footnote{\url{https://github.com/freechipsproject/chisel3}}}
There are two main angles to this question: 1)Chisel improved productivity through new language features, like object oriented programming, functional programming and availability of reusable libraries. 2)Improved specialization due to the hardware-compiler structure. We elaborate more on these two angles in the continue.

Chisel by itself do not provide any new hardware abstractions. However, host language features, Scala, allow designs to be more parameterizable and modular~\cite{izraelevitz_2017_firrtl_reusability}.
For instance,  someone can write a recursive Scala function to construct an adder-reduction tree, parameterized on bit-width.
Unlike the explicitly unrolled version necessary in Verilog, the same generator could be re-used anywhere an adder tree is desired.
Figure~\ref{fig:filter} shows the specified example and the abstract Chisel code.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.45\textwidth]{figures/Introduction/Filter.pdf}
    \caption{Missing Semantics}
    \label{fig:filter}
\end{figure}


\begin{listing}[ht]
    \begin{minted}{Scala}

    abstract class Filter[T <: Data](dtype: T) extends Module {
    val io  = new Bundle {
    val in  = Input(Valid(dtype))
    val out = Output(Valid(dtype))
    } }

    class FunctionFilter[T <: Data](dtype: T, f: T => T) extends Filter(dtype) {
    io.out.valid := io.in.valid
    io.out.bits := f(io.in)
    }

    \end{minted}
    \caption{Chisel abstract function filter}
    \label{listing:chisel_example}
\end{listing}


\begin{listing}[ht]
    \begin{minted}{Scala}

    def clippingFilter[T <: Num](limit: Int, dtype: T) =
        new FunctionFilter(x => min(limit, max(-limit, x)), dtype)

    \end{minted}
    \caption{Chisel abstract function filter}
    \label{listing:chisel_example_f2}
\end{listing}

\begin{listing}[ht]
    \begin{minted}{Scala}

    def shiftingFilter[T <: Num](shift: Int, dtype: T) =
        new FunctionFilter(x => x >> shift, dtype)

    \end{minted}
    \caption{Chisel abstract function filter}
    \label{listing:chisel_example_f3}
\end{listing}


As another example, designer can write a filter module which takes, as a parameter, a higher-order-function that creates the condition checking hardware.
The user of this module then only needs to write the filtering condition, re-using the base filter structure.
There are mature Chisel projects like \emph{Rocket-Chip~\cite{rocket-chip}} and \emph{Diplomacy~\cite{diplomacy}} that can be used as libraries, as examples of Chisel's modularity.
Using these libraries user is being able to \emph{just import a RISC-V microprocessor} in the same way that they \emph{just import a graph library} in software.

The second important advantage of Chisel compare to HDLs is a Hardware Compiler Framework that looks very much like LLVM~\cite{llvm} applied to hardware generation.
The Chisel-to-Verilog process forms part of a multi-stage compiler.
The Chisel front-end compiles Chisel to a circuit intermediate representation called FIRRTL (Flexible Intermediate Representation for RTL)~\cite{firrtl}.
FIRRTL mid-end then optimizes FIRRTL and applies user-custom transformations.
Finally the Verilog back-end emits Verilog based on the optimized FIRRTL.
In this process, FIRRTL represents the standardized elaborated circuit that the Chisel HDL produces.
FIRRTL represents the circuit immediately after Chisel's elaboration but before any circuit simplification.
FIRRTL is designed to resemble the Chisel after all meta-programming has executed. Thus, a user program that makes little use of meta-programming facilities should look almost identical to the generated FIRRT.
In fact, in this architecture Chisel is mostly used for its meta-programming facilities, hence, Chisel front-end can be very light-weight.
Another advantage of such architecture is that other HLD languages, such as Verilog\footnote{Yosys Verilog-to-Firrtl Front-end: \url{https://github.com/cliffordwolf/yosys}} can target FIRRTL and reuse the majority of the compiler toolchain.

While these improvements allow for more powerful meta-programming compared to Verilog \code{generate} statements, users still write programs at a timed circuit level. This is still one of the most important limitation of HCLs to improve the overall \emph{system design productivity}.

\section{Spatial language}
% * Spatial~\cite{david_PLDI_2018_spatial, prabhakar_asplos_2016_parallelpattern}
% * A practice to improve chisel flexibility
% * Higher level of abstraction to chisel
% * Untimed modules
% * Reduction to parallel patterns
Recently, Spatial~\cite{david_PLDI_2018_spatial} is been proposed as a language and compiler for applications specific accelerators.
Spatial focuses on specific type of high-level abstractions required to create a new high level HDL language in which syntax contains memory, control and accelerator-host interface as an individual entities.
Spatial shows these particular constructs within the language are better fit for accelerators specially targeting applications with data locality and data parallelism.
It reduces productivity gap of HDL, structural languages, by increasing the level of abstraction of HDL languages.
In Spatial the design is limited to set of constructs and it allows compiler to optimize the design at the higher-level like optimizing and banking memory modules in the design.
In Spatial, each accelerator is an un-timed module containing  nested loops and customized memory hierarchy. In Listing~\ref{listing:spatial_example} we have shown InnerProduct implementation in Spatial language~\footnote{\url{https://spatial-lang.org/dotprod}}.
In this example, we can see we have two types of memory \code{DRAM} and \code{SRAM}, there is a \code{Reduce} computation pattern and an input mathematic expression.

\begin{listing}[ht]
    \begin{minted}{Scala}

  object InnerProduct extends SpatialApp {
      val d1 = DRAM[T](len)
      val d2 = DRAM[T](len)

      Accel {
        // Create local SRAMs
        val s1 = SRAM[T](len)
        val s2 = SRAM[T](len)
        
        // Transfer data
        s1 load d1
        s2 load d2
        
        // Multiply and accumulate
        x := Reduce(Reg[T](0))(len by 1)
        {i => s1(i) * s2(i)}{_+_} 
      }
    }
}

    \end{minted}
    \caption{Chisel abstract function filter}
    \label{listing:spatial_example}
\end{listing}

In fact, Spatial, is a new DSL language on top of Chisel which tries to strike balance between high-level constructs in the language for improving programmer productivity versus low-level syntax for tuning performance. 
For instance, to enable the compiler to be able to reason about the loop structures, Spatial limits the types of control structures into four types: \code{FSMs}, \code{Foreach}, \code{Reduce} and \code{MapReduce}.
Therefore, as long as the designer can express his design in such patterns, compiler can reason about the available parallelism and automatically pipeline the design.
However, in many cases such control structures are note enough to express arbitrary designs.
While Spatial has improved the accelerator design productivity compare to Chisel.
But Spatial is a structural language, hence, the design needs to be implemented in Spatial language.
Therefore, synthesizing arbitrary designs from behavioral description can be very challenging because of the missing semantics existing between behavioral model, software, and structural model, Spatial~\ref{sec:missing_semantics}.
The main goal of Spatial is limiting DSE by defining higher level abstractions on top of Chisel and enabling compiler to only search trough very limited space.

\section{High-Level Synthesis}

High-level synthesis (HLS) techniques have been proposed to improve the productivity of hardware designers by automatically generating the hardware from a high-level description, behavioral description, of an algorithm.
In pure C-to-gates HLS, front-end captures system behavior with a model of computation in a standard language such as C, C++, SystemC as an input. The language components in this case are in the form of untimed mathematical expressions and nested serial loops, pipeline loops or parallel loops.
In the next step, the compiler statically schedules the input algorithm and applies optimizations such as inner loop pipelining, unrolling, and memory banking and buffering~\cite{chung_micro_2010, lee_1989_new, paulin_1989_force}.
Examples of such HLS tools are LegUp~\cite{canis_2011_legup}, Vivado HLS~\cite{vivadohls}, Intel’s FPGA SDK for OpenCL~\cite{opencl_sdk}, and SDAccel~\cite{sdaccel} allow users to write FPGA designs in C/C++ and OpenCL.
Usually such tools adopt polyhedral tools to automate loop pipelining and banking decisions, but such techniques are limited to only affine accesses withing a single loop nest~\cite{wang_2014_theory}, it does not address non-affine cases or cases where accesses to the same memory occur in multiple loop nests.
For instance, Pouchet et al.~\cite{pouchet_2013_polyhedral}  explore combining HLS with polyhedral analysis to optimize input designs for locality and use estimates from HLS tools to drive design space exploration.
While this captures a larger design space than previous work by including tile sizes, this approach is limited to the capabilities of the HLS tools and to benchmarks that have strictly affine data accesses.

To show an example of such limitation of standard HLS approaches, consider the code in Listing~\ref{listing:static_schedule}.
In this loop there is a control flow decision (if) which depends on the actual data being read from arrays A[] and B[].
The operation which might take place in a specific iteration (s = s + d) introduces a dependency between iterations and delays the next iteration whenever the condition is true.
A typical HLS tool needs to create a static schedule for the loops, which means it needs to take the conservative decisions for synthesizing the loop. The compiler has either two choices, make the loop serial and state machine base or make the loop iterations pipeline, which for pipelining it usually need input directives from the input algorithm.
Figure~\ref{fig:no_pipeline} shows the both possible schedules, serial and pipeline, for \code{if} code when we use HLS tools with statically scheduling approach.


\begin{figure*}[ht]
    \begin{minipage}{0.4\linewidth}
        \begin{minted}{C}
            float d, s = 0.0;
            for (int i = 0; i < 100; i++){
                d = A[i] - B[i];
                if (d >= 0)
                    s += d:
        }
    \end{minted}
    \end{minipage}
    \begin{minipage}{0.45\linewidth}
        \begin{minted}{C}
            float d, s = 0.0;
            #pragma pipeline
            for (int i = 0; i < 100; i++){
                d = A[i] - B[i];
                if (d >= 0)
                    s += d:
        }
    \end{minted}
    \end{minipage}
    \caption{Limitations of static scheduling}
    \label{listing:static_schedule}
\end{figure*}


\begin{figure}[!h]
    \begin{minipage}[t]{\linewidth}
        \centering
        \includegraphics[width=1\textwidth]{figures/Introduction/schedule.pdf}
        \label{fig:no_pipeline}
    \end{minipage}
    \hspace{0.1cm}
    \begin{minipage}[t]{\linewidth} 
        \centering
        \includegraphics[width=1\textwidth]{figures/Introduction/schedule_pipe.pdf}
        \label{fig:pipeline}
    \end{minipage}        
    \label{fig:schedule}
    \caption{Static Schedule: a)no pipeline, b)pipeline}
\end{figure}  



While traditional HLS tools claim they can support any arbitrary C/C++ programs, however, they input can be always suboptimal and then the major challenge in HLS become how to write a good C/C++ code targeting HLS tool to get the optimal performance~\cite{cong_2018_best}.
One of the most challenging part of implementing FIR filter with HLS is having optimizing input C code for an HLS tools~\cite{cong_2018_best}.
This makes it prohibitive to most software programmers. It is even more challenging when the mainstream algorithm in an application domain is constantly evolving; i.e., an algorithm may have already been obsolete during the development process of its hardware accelerator.
For instance in Figure~\ref{listing:hls_fir_filter} we show the implementation of FIR filter, the same implementation of FIR filter in example~\ref{fig:filter} with Chisel.

In this example, if you look at line number 12, you will see a \code{pragma} that directs the compiler to how to implement the \code{for}.
For instance, in this case, have an efficient implementation of \code{for} loop, the designer needs to know what is the minimum, maximum and average \textit{loop trip counts}.
This information is very low level and it is not trivial specially for software programmers.
The reason is that, these types of \code{pragma}s are actually structural information and since the compiler can not find the optimal structural implementation for a behavioral description, therefore, HLS designers decided to embed this information to the higher level abstraction.

\begin{listing}[ht]
    \begin{minted}
        [xleftmargin=20pt, linenos]
        {C}
    #include "fir.h"

    out_data_t fir_filter(inp_data_t x, coef_t c[N]) {
        static inp_data_t shift_reg[N];
    
        acc_t acc = 0;
        acc_t mult;
        out_data_t y;
    
    Shift_Accum_Loop:
        for (int i = N - 1; i >= 0; i--) {
        #pragma HLS LOOP_TRIPCOUNT min = 1 max = 16 avg = 8
    
        if (i == 0) {
            // acc+=x*c[0];
            shift_reg[0] = x;
        } else {
            shift_reg[i] = shift_reg[i - 1];
            // acc+=shift_reg[i]*c[i];
        }
        mult = shift_reg[i] * c[i];
        acc = acc + mult;
        }
    
        y = (out_data_t)acc;
    
        return y;
    }
    \end{minted}
    \caption{FIR filter for HLS}
    \label{listing:hls_fir_filter}
\end{listing}



One solution to come across control dependencies was synthesizing hyperblocks~\cite{hyperblock}.
Through the use of predication, each hyperblock is transformed into straight-line code and then the computation portion of each hyperblock is next implemented speculatively in the form of predicated execution.
CASH~\cite{budiu_cash_2002, budiu_pegasus_2002}, AHRL~\cite{ahrl} and CHiMPS~\cite{chimps} are examples of such technique. But the main disadvantage of this paradigm is the requirement for substantial hardware resources, and it's not a scalable approach. While there are other disadvantages like limitation in memory layout~\cite{spatial_computation}.

\begin{figure}[!h]
    \centering
    \includegraphics[width=1\textwidth]{figures/Introduction/schedule_dynamic.pdf}
    \label{fig:dynamic_schedule}
    \caption{Dynamically schedule}
\end{figure}  



More recent works like Elastic Circuit~\cite{elasticCircuit, elasticFlow}, CGPA~\cite{cgpa} and ~\cite{josipovic_fpga_2018_dynamically} use dynamically schedule circuit to avoid the limitations in inffering loop pipelining.
The idea is to refine from triggering the operators through centralized pre-planned controller but to take scheduling decisions locally in the circuit as it runs.
Therefore, as soon as all the conditions for execution of an operation is satisfied the has to start.
Figure~\ref{fig:dynamic_schedule} shows the execution of a dynamically scheduled HLS circuit.
The key to a good execution of this loop is that, ideally, a new value of \code{i} should be used to start computing \code{A[i] - B[i]} on every cycle.
While this techniques can dynamically provides better schedule compare to other statically scheduled HLS techniques.
But it is clear that, as in the case of processors, taking scheduling decisions dynamically costs resources and time such as the area and delay of the control elements.



Overall all the pure C-to-gates HLS techniques described up to here, rely on capturing parallelism between fine-grain operations of sequential code by constructing the control and dataflow graph (CDFG) of the computation kernels.
Then they use either scheduling algorithms or dynamic techniques to extract parallelism between the operations that are provably independent in the CDFG.
However, these techniques are less effective in capturing coarse-grain parallelism.
Hence, the Quality of Results (QoR) of these HLS tools are often lower than that of manual design process using low-level hardware-description languages.
Because the highest performance hardware exploits both fine-grain and coarse-grain parallelism.

To extract coarse-grain parallelism, main-stream HLS tools accept parallel programming constructs and/or annotations.
For example, CatapultC~\cite{catapult} and Vivado~\cite{vivado} accepts SystemC~\cite{systemc} processes and modules.
Vivado, also, accepts parallel functions/code-blocks communicating through dataflow channels.
Altera openCL compiler~\cite{opencl_sdk} accepts single instruction multiple thread (SIMT) programming model. 
CMOST~\cite{zhang_DAC_2015_cmost} is a C-to-FPGA framework that uses task-level modeling to exploit multi-level parallelism.

ParallelXL~\cite{chen_micro_2018_parallelXL} adopts continuation passing mechanism to express computation as a dynamic task graph with explicit dependencies.
Using continuation framework as foundation, ParallelXL builds other constructs such as data-parallel loops and for-join patterns on top of it.
ParallelXL implementation is a mixture of HCL languages to implement system architecture and standard HLS tools to implement task elements.

In this methodology, however, the number of input/output iterations and the execution time of each dataflow node for processing the input iteration need to be static and known at compiler time.
Due to this static nature of the synchronous data flow (SDF) model, statically scheduled HLS frameworks can explore the space of a design using closed-form formulations, resulting in micro-architectures that achieve throughput and area goals independent of the input data values.
SDF HLS, however, is restricted to regular applications such as signal processing and multi-media.
Examples of such HLS tools with regular data accesses are Rigel~\cite{hegarty_2016_rigel} and Darkroom~\cite{darkroom} which generate Verilog, and PolyMage~\cite{mullapudi_asplos_2015_polymage} that generates OpenMP and C++ for high-level synthesis.
Rigel and Darkroom support generation of specialized memory structures on FPGAs, such as line buffers, in order to capture reuse.

In irregular dataflow applications, however, the memory accesses are usually global and second the processing time of each kernels is dependent on data. 
Gorilla~\cite{lavasani_thesis}++ is a language and compiler which tackle these problems for irregular  data-stream applications.
Gorilla++ language model provides safe and efficient mechanism to access global resources using multi-threading and lock based synchronization.
To overcome on static approach's limitations, the Gorilla compiler uses profile driven optimizations to iteratively refine an accelerator using generic refinement rules for a given input data-set.
Using this approach the generated micro-architecture might not be optimized for other input data-sets however.

Lime~\cite{lime} is a Java-based programming model and runtime from IBM which aims to provide a single unified language to program heterogeneous architectures like FPGAs.
Lime only synthesis a portion of program that recognized as non-hard-to-synthesize.
Lime natively supports custom bit precisions and includes collection operations, with parallelism in such operations inferred by the compiler.
Coarse-grained pipeline and data parallelism are expressed through ``tasks''.
Coarse-grained streaming computation graphs can be constructed using built-in constructs like \texttt{\small{connect}}, \texttt{\small{split}}, and \texttt{\small{join}}.
The Lime runtime system handles buffering, partitioning, and scheduling of stream graphs.
However, coarse-grained pipelines which deviate from the streaming model are not supported, and the programmer has to use a low-level messaging API to handle coarse-grained graphs with feedback loops.
Additionally, the compiler does not perform automatic design tuning.
Finally, the compiler's ability to instantiate banked and buffered memories is unclear as details on banking multi-dimensional data structures for arbitrary access patterns are not specified.

Another example of non-C language base HLS tool is Kiwi~\cite{kiwi}.
Kiwi translates a set of $C\#$ parallel constructs (e.g., event, monitor, and lock) to corresponding hardware units.